{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from common.data import load_data\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../data/5. Battery Data Set/1. BatteryAgingARC-FY08Q4\")\n",
    "\n",
    "def make_paths(names: Iterable[str]):\n",
    "    return [\n",
    "        base_path.joinpath(name)\n",
    "        for name in names\n",
    "    ]\n",
    "\n",
    "train_paths = make_paths([\"B0005.mat\", \"B0006.mat\"])\n",
    "valid_paths = make_paths([\"B0007.mat\"])\n",
    "test_paths = make_paths([\"B0018.mat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paths(paths: Iterable[Path]) -> pd.DataFrame:\n",
    "    data = pd.concat({Path(p).stem: load_data(p, \"discharge\") for p in paths})\n",
    "    data.index.names = [\"file\", \"index\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_paths(train_paths)\n",
    "valid_data = load_paths(valid_paths)\n",
    "test_data = load_paths(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_dataframe(df: pd.DataFrame, size: int) -> pd.DataFrame:\n",
    "    windows = []\n",
    "    for s in range(size):\n",
    "        shifted = df.shift(s)\n",
    "        shifted.columns = shifted.columns.map(lambda c: f\"{c}_b{s}\")\n",
    "        windows.append(shifted)\n",
    "    return pd.concat(windows, axis=1).iloc[size-1:].reset_index(drop=True)\n",
    "\n",
    "def process_data(data: pd.DataFrame, window_size: int, rolling_size=20):\n",
    "    X = []\n",
    "    y = []\n",
    "    etc = []\n",
    "\n",
    "    for _, group_df in data.groupby(\"file\"):\n",
    "        operation_df = group_df.groupby(\"operation_id\").agg({\n",
    "            \"Time\": [\"max\"],\n",
    "            \"Capacity\": [\"first\"],\n",
    "        })\n",
    "        operation_df.columns = operation_df.columns.map(lambda c: \"_\".join(c))\n",
    "\n",
    "        capacity_change = operation_df[\"Capacity_first\"].rolling(rolling_size).mean().diff().shift(-1)\n",
    "        time_change = operation_df[\"Time_max\"].rolling(rolling_size).mean().diff().shift(-1)\n",
    "\n",
    "        is_alive = operation_df.eval(\"Capacity_first > 1.4\")\n",
    "        alive_cycles = is_alive.sum()\n",
    "        rul_cycles = -(np.arange(len(is_alive)) - alive_cycles)\n",
    "        \n",
    "        X.append(operation_df.iloc[rolling_size-1:-1])\n",
    "        y.append(pd.DataFrame({\n",
    "            \"time\": time_change.iloc[rolling_size-1:-1],\n",
    "            \"cap\": capacity_change.iloc[rolling_size-1:-1],\n",
    "        }))\n",
    "        etc.append(pd.DataFrame({\n",
    "            \"rul\": rul_cycles[rolling_size-1:-1],\n",
    "        }))\n",
    "\n",
    "    X = pd.concat(X, ignore_index=True)\n",
    "    y = pd.concat(y, ignore_index=True)\n",
    "    etc = pd.concat(etc, ignore_index=True)\n",
    "\n",
    "    X_win = window_dataframe(X, window_size)\n",
    "    y_win = y.iloc[window_size-1:].reset_index(drop=True)\n",
    "    etc_win = etc[window_size-1:].reset_index(drop=True)\n",
    "\n",
    "    return X_win, y_win, etc_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "train_X, train_y, train_etc = process_data(train_data, window_size=window_size)\n",
    "valid_X, valid_y, valid_etc = process_data(valid_data, window_size=window_size)\n",
    "test_X, test_y, test_etc = process_data(test_data, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScaler().fit(train_X)\n",
    "train_X_norm = norm.transform(train_X)\n",
    "valid_X_norm = norm.transform(valid_X)\n",
    "test_X_norm = norm.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    target: LGBMRegressor(verbose=-1).fit(\n",
    "        train_X_norm, train_y[target],\n",
    "        eval_set=(valid_X_norm, valid_y[target])\n",
    "    )\n",
    "    for target in train_y.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(y, pred):\n",
    "    plot_min = min(pred.min(), y.min())\n",
    "    plot_max = min(pred.max(), y.max())\n",
    "\n",
    "    plt.scatter(pred, y, alpha=0.2)\n",
    "    plt.plot(\n",
    "        [plot_min, plot_max], [plot_min, plot_max], \n",
    "        color=\"orange\", \n",
    "        linestyle=\"dashed\",\n",
    "    )\n",
    "    plt.title(\"Prediction vs Real\")\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def regression_report(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "\n",
    "    print(f\"rmse: {mean_squared_error(y, pred):0.4f}\")\n",
    "    plot_prediction(y, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, model in models.items():\n",
    "    print(target)\n",
    "    regression_report(model, train_X_norm, train_y[target]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, model in models.items():\n",
    "    print(target)\n",
    "    regression_report(model, valid_X_norm, valid_y[target]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, model in models.items():\n",
    "    print(target)\n",
    "    regression_report(model, test_X_norm, test_y[target]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(models, norm, X: pd.DataFrame, life_window: int):\n",
    "    curr_X = X\n",
    "    for _ in range(life_window):\n",
    "        X_norm = norm.transform(curr_X)\n",
    "        preds = [\n",
    "            model.predict(X_norm)\n",
    "            for _, model in models.items()\n",
    "        ]\n",
    "        new_values = curr_X.iloc[:, :2] + np.column_stack(preds) \n",
    "        new_features = np.column_stack((new_values, curr_X.iloc[:, :-2].values))\n",
    "        curr_X = pd.DataFrame(new_features, columns=X.columns)\n",
    "\n",
    "    return curr_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_simulation(models, norm, X, etc, life_window):\n",
    "    sim_res = run_simulation(models, norm, X, 30)\n",
    "    dead_sim = sim_res[\"Capacity_first_b0\"] < 1.4\n",
    "    dead_rul = etc[\"rul\"] < 30\n",
    "    print(classification_report(dead_rul, dead_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_simulation(models, norm, train_X, train_etc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_simulation(models, norm, valid_X, valid_etc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_simulation(models, norm, test_X, test_etc, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
